{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9682745,"sourceType":"datasetVersion","datasetId":5918750}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download and Imports","metadata":{}},{"cell_type":"code","source":"# Standard Libraries\nimport os\nimport random\nimport shutil\nimport copy\nimport warnings\nimport optuna\nfrom pathlib import Path\n\n# Numerical and Data Manipulation Libraries\nimport numpy as np\n\n# Image Processing Libraries\nimport cv2\nfrom PIL import Image\n\n# Progress Bar\nfrom tqdm import tqdm\n\n# PyTorch Libraries\nimport torch\n\n# TensorFlow and Keras Libraries\nimport tensorflow as tf\nfrom tensorflow.keras import layers, backend as K\nfrom tensorflow.keras.utils import register_keras_serializable\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, AdamW, SGD, RMSprop\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.layers import (\n    Layer, Input, Dense, Dropout, LayerNormalization, Flatten, Concatenate, multiply, BatchNormalization,\n    Conv2D, GlobalMaxPooling2D, GlobalAveragePooling2D, Add, Multiply, MultiHeadAttention, Reshape, Permute, Activation, concatenate\n)\nfrom tensorflow.keras.applications import DenseNet201\n\n# Machine Learning and Evaluation Libraries\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_curve, auc,\n    accuracy_score, mean_squared_error\n)\nfrom imblearn.over_sampling import SMOTE\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\n# Initialize Plotly for notebook\ninit_notebook_mode(connected=True)\n\n# Remove Warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set Random Seed for Reproducibility\nRANDOM_SEED = 123\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing the path to dataset\ndata_dir = Path(\"/kaggle/input/breakhiss/HPI/100X\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total image count inside the dataset\nimage_count = len(list(data_dir.glob('**/*.png')))\nprint(image_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Iterate over subfolders and count images in each subfolder\nfor subfolder in data_dir.glob('*'):\n    if subfolder.is_dir():  # checks if it's a subfolder\n        subfolder_name = subfolder.name\n        image_count = len(list(subfolder.glob('*.png')))\n        print(f\"Folder: {subfolder_name}, Image Count: {image_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the image size you want\nIMG_SIZE = (224, 224)\n\nNUM_CLASSES = 2\n\n# List of subfolders (classes) in the dataset directory\nclasses = ['Benign', 'Malignant']\n\n# Move the model to GPU if avilable\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install tree\n#clear_output()\n# create new folders\n!mkdir TRAIN TEST TRAIN/BENIGN TRAIN/MALIGNANT TEST/BENIGN TEST/MALIGNANT\n!tree -d","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train - Test Split","metadata":{}},{"cell_type":"code","source":"IMG_PATH = '/kaggle/input/breakhiss/HPI/400X/'\nTRAIN_PERCENT = 0.7\nTEST_PERCENT = 0.3\n\n# split the data by train/val/test\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '/' + FILE_NAME\n            if n < TRAIN_PERCENT * IMG_NUM:\n                shutil.copy(img, 'TRAIN/' + CLASS.upper() + '/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function to load images from directory","metadata":{}},{"cell_type":"code","source":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '/' + file)\n                    img = cv2.resize(img, img_size)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_DIR = 'TRAIN/'\nTEST_DIR = 'TEST/'\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the shapes\nprint(\"X_Train Shape:\", X_train.shape)\nprint(\"y_train Shape:\", y_train.shape)\n# Check the data type of the preprocessed images\nprint(f\"Data type of HP images (X_train): {X_train.dtype}\")\nprint(f\"Min pixel value: {np.min(X_train)}\")\nprint(f\"Max pixel value: {np.max(X_train)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have a list of class labels, e.g., ['Benign', 'Malignant']\nclass_labels = set(y_test)  # Assuming 'y_resampled' contains your class labels\n\n# Iterate through each class label and count images\nfor label in class_labels:\n    image_count = sum(1 for item in y_test if item == label)\n    print(f'Training Folder: {label}, Image Count: {image_count}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the shapes\nprint(\"X_test Shape:\", X_test.shape)\nprint(\"y_test Shape:\", y_test.shape)\n# Check the data type of the preprocessed images\nprint(f\"Data type of HP images (X_test): {X_test.dtype}\")\nprint(f\"Min pixel value: {np.min(X_test)}\")\nprint(f\"Max pixel value: {np.max(X_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bar graph plot for visualization of distribution of dataset","metadata":{}},{"cell_type":"code","source":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Test Set'],\n    y=y[0],\n    name='Benign',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Test Set'],\n    y=y[1],\n    name='Malignant',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function to plot samples from each class","metadata":{}},{"cell_type":"code","source":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n/j)\n\n        plt.figure(figsize=(15,2))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_samples(X_train, y_train, labels, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Demo augmentation with one sample image","metadata":{}},{"cell_type":"code","source":"os.makedirs('preview', exist_ok=True)\nx = X_train[0]  \nx = x.reshape((1,) + x.shape) \n\n# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='png'):\n    i += 1\n    if i > 20:\n        break ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(X_train[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview/'):\n    img = cv2.imread('preview/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Augmentation","metadata":{}},{"cell_type":"code","source":"# Define benign and malignant images\nbenign_images = X_train[y_train == 0]\nmalignant_images = X_train[y_train == 1]\n\n# Define augmentation factors\nbenign_augmentation_factor = 4\nmalignant_augmentation_factor = 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation using ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=40,                # Randomly rotate images by up to 40 degrees\n    width_shift_range=0.2,            # Randomly shift images horizontally by 20% of the width\n    height_shift_range=0.2,           # Randomly shift images vertically by 20% of the height\n    shear_range=0.2,                  # Randomly apply shear transformations\n    zoom_range=0.2,                   # Randomly zoom in on images\n    horizontal_flip=True,             # Randomly flip images horizontally\n    vertical_flip=True,               # Randomly flip images vertically\n    preprocessing_function=lambda x: x + np.random.normal(0, 0.25, x.shape),  # Add Gaussian noise (mean=0, variance=0.25)\n    fill_mode='nearest'               # Fill in any empty pixels after transformations with the nearest valid value\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to apply augmentation, resize, and save images\ndef aug_images(image_list, label, augmentation_factor, label_name):\n    all_images = []\n    all_labels = []\n\n    for image in image_list:\n        all_images.append(image)\n        all_labels.append(label)\n\n    # Generate augmented images on-the-fly\n    augmented_data = datagen.flow(np.array(all_images), np.array(all_labels), batch_size=len(all_images), shuffle=False)\n    # Get augmented images and labels\n    X_augmented, y_augmented = next(augmented_data)\n    # Save augmented images\n    augmented_path = f\"Augmented/Augmented_{label_name}\"\n    os.makedirs(augmented_path, exist_ok=True)\n\n    for i in range(len(all_images)):\n        for j in range(augmentation_factor):\n            index = i * augmentation_factor + j\n            cv2.imwrite(os.path.join(augmented_path, f'aug_{index+1}.png'), X_augmented[index % len(X_augmented)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process Benign Images\naug_images(benign_images, 0, benign_augmentation_factor, 'Benign')\n\n# Process Malignant Images\naug_images(malignant_images, 1, malignant_augmentation_factor, 'Malignant')\n\nprint(\"Augmentation completed successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmented_dir = Path(\"/kaggle/working/Augmented\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Iterate over subfolders and count images in each subfolder\nfor subfolder in augmented_dir.glob('*'):\n    if subfolder.is_dir():  # checks if it's a subfolder\n        subfolder_name = subfolder.name\n        image_count = len(list(subfolder.glob('*.png')))\n        print(f\"Folder: {subfolder_name}, Image Count: {image_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Balance the augmented dataset using SMOTE(Synthetic Minority Oversampling Technique)","metadata":{}},{"cell_type":"code","source":"Augumented_Img_Dir = \"/kaggle/working/Augmented/\"\n\n# Initialize empty lists to store images and labels\nX = []  # This will store image data\ny = []  # This will store corresponding labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# All subfolders are appended to achieve a more balanced dataset\nfor subfolder in os.listdir(Augumented_Img_Dir):\n    if os.path.isdir(os.path.join(Augumented_Img_Dir, subfolder)):\n        label = subfolder\n        subfolder_path = os.path.join(Augumented_Img_Dir, subfolder)\n\n        # Iterate through images in the subfolder\n        for image_file in os.listdir(subfolder_path):\n            image_path = os.path.join(subfolder_path, image_file)\n\n            try:\n                # Load the image and append it to X\n                image = Image.open(image_path)\n                X.append(np.array(image))  # Convert image to numpy array\n\n                # Append the label to y\n                y.append(label)\n            except Exception as e:\n                print(f\"Error loading image: {image_path}\")\n                print(f\"Error message: {str(e)}\")\n\nprint(\"Total images loaded:\", len(X))\nprint(\"Total labels loaded:\", len(y))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert X and y to numpy arrays\nX = np.array(X)\ny = np.array(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshape each image to a flat 1D array\nX_flat = [image.flatten() for image in X]\n# Convert the list of flat arrays to a 2D NumPy array\nX_flat = np.array(X_flat)\n\n# Apply SMOTE to balance the dataset\nsmote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5)\nX_resampled, y_resampled = smote.fit_resample(X_flat, y)\n\n# Reshape the flattened images back to their original shape\nX_balanced = X_resampled.reshape(-1, IMG_SIZE[0], IMG_SIZE[1], 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have a list of class labels, e.g., ['Benign', 'Malignant']\nclass_labels = set(y_resampled)  # Assuming 'y_resampled' contains your class labels\n\n# Iterate through each class label and count images\nfor label in class_labels:\n    image_count = sum(1 for item in y_resampled if item == label)\n    print(f'Training Folder: {label}, Image Count: {image_count}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming X_balanced contains images after balancing\n# Plot a few balanced images to check color correctness and data type\nnum_images_to_plot = 5\nfor i in range(num_images_to_plot):\n    plt.subplot(1, num_images_to_plot, i + 1)\n    plt.imshow(X_balanced[i])  # Display the image\n    plt.title(f\"Image {i + 1}\")\n    plt.axis(\"off\")\nplt.show()\n\n# Check the data type of the balanced images\nprint(f\"Data type of balanced images (X_balanced): {X_balanced.dtype}\")\nprint(f\"Min pixel value: {np.min(X_balanced)}\")\nprint(f\"Max pixel value: {np.max(X_balanced)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the images after balancing","metadata":{}},{"cell_type":"code","source":"def save_img(images, labels, classes, save_dir):\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    for idx, (img, label) in enumerate(zip(images, labels)):\n        class_dir = save_dir / classes[label]\n        class_dir.mkdir(parents=True, exist_ok=True)\n        img_path = class_dir / f\"image_{idx + 1}.png\"\n        cv2.imwrite(str(img_path), img)\n    print(f\"Images saved to {save_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aug_classes = ['Augmented_Benign', 'Augmented_Malignant']\n\n# Map class names to numeric labels\nlabel_mapping = {class_name: idx for idx, class_name in enumerate(aug_classes)}\n\n# Convert y_resampled to numeric labels using the mapping\ny_resampled_numeric = [label_mapping[label] for label in y_resampled]\n\n# Save the Balanced images\nsave_img(X_balanced, y_resampled_numeric, aug_classes, \"/kaggle/working/Balanced_HP_Images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the shape of balanced dataset\nprint(\"X_balanced Shape:\", X_balanced.shape)\nprint(\"y_resampled_numeric Shape:\", y_resampled.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the augmented-balaced dataset for training","metadata":{}},{"cell_type":"code","source":"TRAINING_DIR = \"/kaggle/working/Balanced_HP_Images/\"\n\n# use predefined function to load the image data into workspace\nX_training, y_training, labels = load_data(TRAINING_DIR, IMG_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_samples(X_training, y_training, labels, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the shapes of the training  sets\nprint(\"X_training shape:\", X_training.shape)\nprint(\"y_training shape:\", y_training.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the data type of the preprocessed_augmented images\nprint(f\"Data type of HP images (X_training): {X_training.dtype}\")\nprint(f\"Min pixel value: {np.min(X_training)}\")\nprint(f\"Max pixel value: {np.max(X_training)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Validation split using StratifiedShuffleSplit","metadata":{}},{"cell_type":"code","source":"# Define the percentages for training and validation sets\nTRAIN_PERCENT = 0.7  # 70% for training\nVAL_PERCENT = 0.3    # 30% for validation\n\n# Initialize StratifiedShuffleSplit\nstratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=VAL_PERCENT, random_state=42)\n\n# Perform stratified splitting\nfor train_index, val_index in stratified_splitter.split(X_training, y_training):\n    X_Train, y_Train = X_training[train_index], y_training[train_index]\n    X_val, y_val = X_training[val_index], y_training[val_index]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Visualization of Class Distribution Across Datasets","metadata":{}},{"cell_type":"code","source":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_Train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='Benign',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Malignant',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the shapes of the training and validation sets\nprint(\"X_Train shape:\", X_Train.shape)\nprint(\"y_Train shape:\", y_Train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the data type of the X_Train images\nprint(f\"Data type of HP images (X_Train): {X_Train.dtype}\")\nprint(f\"Min pixel value: {np.min(X_Train)}\")\nprint(f\"Max pixel value: {np.max(X_Train)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the images to training and validation directory","metadata":{}},{"cell_type":"code","source":"# Create the directories if they don't exist\nos.makedirs('Training_Dir', exist_ok=True)\nos.makedirs('Validation_Dir', exist_ok=True)\n\n# Use predefined function to load the image data into workspace\n# Load resized images as np.arrays to the workspace\ndef save_images(directory, X, y, img_size=(100, 100)):\n    \"\"\"\n    Save resized images as np.arrays to the specified directory\n    \"\"\"\n    for i in tqdm(range(len(X))):\n        label = labels[y[i]]\n        img_name = f\"{label}_{i}.png\"\n        img_path = os.path.join(directory, label, img_name)\n        os.makedirs(os.path.dirname(img_path), exist_ok=True)\n        img = cv2.resize(X[i], img_size)\n        cv2.imwrite(img_path, img)\n\n# Save training images\nsave_images('Training_Dir', X_Train, y_Train, IMG_SIZE)\n\n# Save validation images\nsave_images('Validation_Dir', X_val, y_val, IMG_SIZE)\n\nprint(\"Images saved successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining ImageDataGenerator for Training, Validation, and Testing","metadata":{}},{"cell_type":"code","source":"# Define directories for training, validation, and test data\nTRAINING_DIR = 'Training_Dir/'\nVALIDATION_DIR = 'Validation_Dir/'\nTEST_DIR = 'TEST/'\n\n# Create ImageDataGenerator for data augmentation and preprocessing for training data\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,                  # Rescale pixel values to [0, 1] range\n    rotation_range=20,               # Rotate images by up to 20 degrees\n    width_shift_range=0.1,           # Shift images horizontally by up to 10%\n    height_shift_range=0.1,          # Shift images vertically by up to 10%\n    shear_range=0.15,                # Apply shear transformation\n    zoom_range=0.15,                 # Apply zoom transformation\n    horizontal_flip=True,            # Randomly flip images horizontally\n    vertical_flip=False,             # Do not randomly flip images vertically\n    fill_mode='nearest'              # Fill in missing pixels after transformation\n)\n\n# Create ImageDataGenerator for validation and test data preprocessing (no augmentation, only rescaling)\ntest_datagen = ImageDataGenerator(\n    rescale=1./255  # Rescale pixel values to [0, 1] range\n)\n\n# Create training data generator\ntrain_generator = train_datagen.flow_from_directory(\n    TRAINING_DIR,                    # Directory containing training images\n    color_mode='rgb',                # Use RGB images\n    target_size=IMG_SIZE,            # Resize images to target size\n    batch_size=16,                   # Number of images to return in each batch\n    class_mode='categorical',        # Return one-hot encoded labels\n    shuffle=True,                    # Shuffle images randomly\n    seed=RANDOM_SEED                 # Set random seed for reproducibility\n)\n\n# Create validation data generator\nvalidation_generator = test_datagen.flow_from_directory(\n    VALIDATION_DIR,                  # Directory containing validation images\n    color_mode='rgb',                # Use RGB images\n    target_size=IMG_SIZE,            # Resize images to target size\n    batch_size=16,                   # Number of images to return in each batch\n    class_mode='categorical',        # Return one-hot encoded labels\n    shuffle=False,                   # Do not shuffle validation data\n    seed=RANDOM_SEED                 # Set random seed for reproducibility\n)\n\n# Create test data generator\ntest_generator = test_datagen.flow_from_directory(\n    TEST_DIR,                        # Directory containing test images\n    color_mode='rgb',                # Use RGB images\n    target_size=IMG_SIZE,            # Resize images to target size\n    batch_size=8,                   # Number of images to return in each batch\n    class_mode='categorical',        # Return one-hot encoded labels\n    shuffle=False,                   # Ensure consistent order of test data\n    seed=RANDOM_SEED                 # Set random seed for reproducibility\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Propsed Model Design","metadata":{}},{"cell_type":"code","source":"@register_keras_serializable()\nclass EnhancedAttentionMechanism(Layer):\n    def __init__(self, filters, reduction=8, num_heads=3, dropout_rate=0.3, **kwargs):\n        \"\"\"\n        Enhanced Attention Mechanism (EAM) with Multi-Head Self-Attention (MHSA).\n        \n        Parameters:\n        - filters: Number of filters in the input tensor.\n        - reduction: Reduction ratio for channel attention.\n        - num_heads: Number of attention heads for MHSA.\n        - dropout_rate: Dropout rate for regularization.\n        \"\"\"\n        super(EnhancedAttentionMechanism, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction = reduction\n        self.num_heads = num_heads\n        self.dropout_rate = dropout_rate\n\n        # Channel Attention components\n        self.global_max_pool = GlobalMaxPooling2D()\n        self.dense1 = Dense(filters // reduction, activation='relu', kernel_regularizer=l2(1e-3))\n        self.dense2 = Dense(filters, activation='sigmoid', kernel_regularizer=l2(1e-3))\n\n        # Spatial Attention components\n        self.conv_spatial = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n\n        # Multi-Head Self-Attention (MHSA)\n        self.mhsa = MultiHeadAttention(num_heads=num_heads, key_dim=filters)\n\n        # Dropout for regularization\n        self.dropout = Dropout(dropout_rate)\n\n    def call(self, inputs):\n        # Channel Attention (Global Context)\n        avg_pool = self.global_max_pool(inputs)\n        channel_attention = self.dense1(avg_pool)\n        channel_attention = self.dense2(channel_attention)\n        channel_attention = Reshape((1, 1, self.filters))(channel_attention)\n        channel_refined = Multiply()([inputs, channel_attention])\n\n        # Spatial Attention (Local Features)\n        spatial_attention = self.conv_spatial(channel_refined)\n        spatial_refined = Multiply()([inputs, spatial_attention])\n\n        # Multi-Head Self-Attention (Long-Range Dependencies)\n        # Flatten spatial dimensions to allow MHSA processing\n        batch_size, height, width, channels = inputs.shape\n        flattened_inputs = Reshape((height * width, channels))(inputs)\n        mhsa_output = self.mhsa(flattened_inputs, flattened_inputs)\n        mhsa_output = Reshape((height, width, channels))(mhsa_output)\n\n        # Combine Channel, Spatial, and MHSA Outputs\n        combined = Add()([channel_refined, spatial_refined, mhsa_output])\n        return self.dropout(combined)\n\n    def get_config(self):\n        config = super(EnhancedAttentionMechanism, self).get_config()\n        config.update({\n            \"filters\": self.filters,\n            \"reduction\": self.reduction,\n            \"num_heads\": self.num_heads,\n            \"dropout_rate\": self.dropout_rate,\n        })\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the DenseNet model without the top fully connected layers, using pre-trained ImageNet weights\nbase_model = DenseNet201(include_top=False, weights='imagenet', input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3))\n\n# Save the input and output tensors of the base model\nbase_in = base_model.input\nbase_out = base_model.output\n\n# Apply layer normalization to the output of the base model\nx = LayerNormalization(epsilon=1e-6)(base_out)\n\n# Initialize a custom self-attention layer with specified parameters\neam_layer = EnhancedAttentionMechanism(filters=x.shape[-1])\n\n# Apply the self-attention layer to the normalized output\neam_op = eam_layer(x)\n\n# Add the backbone output and the self-attention output (residual connection)\ny = Add()([base_out, eam_op])\n\n# Apply another layer normalization to the self-attention output\nx = LayerNormalization(epsilon=1e-6)(y)\n\n# Apply a 1x1 convolution with filters, l2 regularizer and ReLU activation\nx = Conv2D(filters=x.shape[-1], kernel_size=1, padding='same', activation='relu')(x)\n\n# Add the convolution output and the previous addition output (residual connection)\nx = Add()([x, y])\n\n# Apply global average pooling to reduce each feature map to a single value\nx = GlobalAveragePooling2D()(x)\n\n# Add a fully connected layer with softmax activation for classification\nout = Dense(NUM_CLASSES, activation='softmax')(x)\n\n# Create the final model from the input of the base model to the output layer\nmodel = Model(base_in, out, name=\"DREAMNet\")\n    \n# Print the summary of the model\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define callbacks","metadata":{}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.7,\n    patience=3,\n    min_lr=1e-6,\n    verbose=1\n)\n\ncheckpoint = ModelCheckpoint(\n    'best_model.keras', \n    monitor='val_accuracy', \n    save_best_only=True, \n    mode='max',\n    verbose=1\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss', \n    patience=5, \n    restore_best_weights=True, \n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Two stage model training","metadata":{}},{"cell_type":"code","source":"# Set all layers in the model to be non-trainable (freeze the model)\nfor layer in base_model.layers:\n    layer.trainable = False\n    \n# Compile the model\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adam(learning_rate=0.005, decay=1e-6),\n    metrics=['accuracy']\n)\n\n# Fit the model with callbacks\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=2,\n    callbacks=[reduce_lr, checkpoint, early_stopping],\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure that all layers in the model (including custom layers) are trainable\nfor layer in base_model.layers:\n    layer.trainable = True\n    \n# Compile the model\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adam(learning_rate=1e-4, decay=1e-5),\n    metrics=['accuracy']\n)\n\n# Fit the model with callbacks\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=50,\n    callbacks=[reduce_lr, checkpoint, early_stopping],\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Graph plots","metadata":{}},{"cell_type":"code","source":"# Plot training and validation loss and accuracy curves\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and evaluation of the model with different metrics","metadata":{}},{"cell_type":"code","source":"# Step 1: Load the best model\nbest_model = load_model('best_model.keras', custom_objects={'EnhancedAttentionMechanism': EnhancedAttentionMechanism})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Evaluate the model on test data\ntest_loss, test_accuracy = best_model.evaluate(test_generator)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Generate predictions for the test set\npredictions = best_model.predict(test_generator)\ntrue_classes = test_generator.classes  # True labels\nclass_labels = list(test_generator.class_indices.keys())  # Class names\n\n# Convert predictions to binary class labels\npredicted_classes = np.argmax(predictions, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Confusion Matrix\ncm = confusion_matrix(true_classes, predicted_classes)\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Classification Report\nprint(\"Classification Report:\\n\", classification_report(true_classes, predicted_classes, target_names=class_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: AUC and ROC Curve\n# Get probability scores for the positive class (assuming class 1 is the positive class)\npositive_class_probs = predictions[:, 1]\nfpr, tpr, thresholds = roc_curve(true_classes, positive_class_probs)\nroc_auc = auc(fpr, tpr)\nprint(f\"AUC Score: {roc_auc:.4f}\")\n\n# Plot ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mean_squared_error(true_classes, predicted_classes))\nprint(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Happy Coding!","metadata":{}}]}
